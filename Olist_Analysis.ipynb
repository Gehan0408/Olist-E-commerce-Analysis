{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19037d6f-8d16-42b7-a951-89f43c43e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "orders: (99441, 8)\n",
      "order_items: (112650, 7)\n",
      "customers: (99441, 5)\n",
      "products: (32951, 9)\n",
      "payments: (103886, 5)\n",
      "reviews: (99224, 7)\n",
      "sellers: (3095, 4)\n",
      "geolocation: (1000163, 5)\n",
      "\n",
      "Orders dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99441 entries, 0 to 99440\n",
      "Data columns (total 8 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   order_id                       99441 non-null  object\n",
      " 1   customer_id                    99441 non-null  object\n",
      " 2   order_status                   99441 non-null  object\n",
      " 3   order_purchase_timestamp       99441 non-null  object\n",
      " 4   order_approved_at              99281 non-null  object\n",
      " 5   order_delivered_carrier_date   97658 non-null  object\n",
      " 6   order_delivered_customer_date  96476 non-null  object\n",
      " 7   order_estimated_delivery_date  99441 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 6.1+ MB\n",
      "None\n",
      "\n",
      "Orders head:\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
      "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
      "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
      "\n",
      "  order_estimated_delivery_date  \n",
      "0           2017-10-18 00:00:00  \n",
      "1           2018-08-13 00:00:00  \n",
      "2           2018-09-04 00:00:00  \n",
      "3           2017-12-15 00:00:00  \n",
      "4           2018-02-26 00:00:00  \n",
      "\n",
      "Order status distribution:\n",
      "order_status\n",
      "delivered      96478\n",
      "shipped         1107\n",
      "canceled         625\n",
      "unavailable      609\n",
      "invoiced         314\n",
      "processing       301\n",
      "created            5\n",
      "approved           2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date range check:\n",
      "Date range: 2016-09-04 21:15:19 to 2018-10-17 17:30:18\n",
      "\n",
      "Missing values summary:\n",
      "orders: 4908 missing values\n",
      "order_approved_at                 160\n",
      "order_delivered_carrier_date     1783\n",
      "order_delivered_customer_date    2965\n",
      "dtype: int64\n",
      "------------------------------\n",
      "products: 2448 missing values\n",
      "product_category_name         610\n",
      "product_name_lenght           610\n",
      "product_description_lenght    610\n",
      "product_photos_qty            610\n",
      "product_weight_g                2\n",
      "product_length_cm               2\n",
      "product_height_cm               2\n",
      "product_width_cm                2\n",
      "dtype: int64\n",
      "------------------------------\n",
      "reviews: 145903 missing values\n",
      "review_comment_title      87656\n",
      "review_comment_message    58247\n",
      "dtype: int64\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Initial Data Exploration and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load all datasets\n",
    "path = 'data/'\n",
    "orders = pd.read_csv(path+'olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv(path+'olist_order_items_dataset.csv')\n",
    "customers = pd.read_csv(path+'olist_customers_dataset.csv')\n",
    "products = pd.read_csv(path+'olist_products_dataset.csv')\n",
    "payments = pd.read_csv(path+'olist_order_payments_dataset.csv')\n",
    "reviews = pd.read_csv(path+'olist_order_reviews_dataset.csv')\n",
    "sellers = pd.read_csv(path+'olist_sellers_dataset.csv')\n",
    "geolocation = pd.read_csv(path+'olist_geolocation_dataset.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "datasets = {\n",
    "    'orders': orders,\n",
    "    'order_items': order_items,\n",
    "    'customers': customers,\n",
    "    'products': products,\n",
    "    'payments': payments,\n",
    "    'reviews': reviews,\n",
    "    'sellers': sellers,\n",
    "    'geolocation': geolocation\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "\n",
    "print(\"\\nOrders dataset info:\")\n",
    "print(orders.info())\n",
    "print(\"\\nOrders head:\")\n",
    "print(orders.head())\n",
    "\n",
    "print(\"\\nOrder status distribution:\")\n",
    "print(orders['order_status'].value_counts())\n",
    "\n",
    "print(\"\\nDate range check:\")\n",
    "orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "print(f\"Date range: {orders['order_purchase_timestamp'].min()} to {orders['order_purchase_timestamp'].max()}\")\n",
    "\n",
    "# Check for missing values across all datasets\n",
    "print(\"\\nMissing values summary:\")\n",
    "for name, df in datasets.items():\n",
    "    missing = df.isnull().sum().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"{name}: {missing} missing values\")\n",
    "        print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7cc704-7b22-4bc1-b3c7-ad7641d4f5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original orders: 99441\n",
      "Delivered orders: 96478\n",
      "Orders with delivery dates: 96470\n",
      "Duplicate order IDs: 0\n",
      "Order items after filtering: 110189\n",
      "Payment records after filtering: 100748\n",
      "Reviews after filtering: 96353\n",
      "Geolocation records after cleaning: 19007\n",
      "\n",
      "Data integrity checks:\n",
      "Orders with items: 96470\n",
      "Orders with payments: 96469\n",
      "Orders with reviews: 95824\n",
      "\n",
      "Cleaning completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Data Cleaning and Quality Checks\n",
    "\n",
    "# Focus on delivered orders only for meaningful analysis\n",
    "print(f\"Original orders: {len(orders)}\")\n",
    "orders_clean = orders[orders['order_status'] == 'delivered'].copy()\n",
    "print(f\"Delivered orders: {len(orders_clean)}\")\n",
    "\n",
    "# Convert date columns\n",
    "date_columns = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date']\n",
    "for col in date_columns:\n",
    "    if col in orders_clean.columns:\n",
    "        orders_clean[col] = pd.to_datetime(orders_clean[col])\n",
    "\n",
    "# Remove orders with missing delivery dates since we need them for analysis\n",
    "orders_clean = orders_clean.dropna(subset=['order_delivered_customer_date'])\n",
    "print(f\"Orders with delivery dates: {len(orders_clean)}\")\n",
    "\n",
    "# Check for duplicate orders\n",
    "duplicates = orders_clean['order_id'].duplicated().sum()\n",
    "print(f\"Duplicate order IDs: {duplicates}\")\n",
    "\n",
    "# Filter order items to match our clean orders\n",
    "order_items_clean = order_items[order_items['order_id'].isin(orders_clean['order_id'])].copy()\n",
    "print(f\"Order items after filtering: {len(order_items_clean)}\")\n",
    "\n",
    "# Clean payments data\n",
    "payments_clean = payments[payments['order_id'].isin(orders_clean['order_id'])].copy()\n",
    "print(f\"Payment records after filtering: {len(payments_clean)}\")\n",
    "\n",
    "# Check payment consistency\n",
    "payment_summary = payments_clean.groupby('order_id')['payment_value'].sum().reset_index()\n",
    "payment_summary.columns = ['order_id', 'total_payment']\n",
    "\n",
    "# Clean reviews data\n",
    "reviews_clean = reviews[reviews['order_id'].isin(orders_clean['order_id'])].copy()\n",
    "reviews_clean['review_creation_date'] = pd.to_datetime(reviews_clean['review_creation_date'])\n",
    "print(f\"Reviews after filtering: {len(reviews_clean)}\")\n",
    "\n",
    "# Handle missing review scores (fill with median)\n",
    "median_score = reviews_clean['review_score'].median()\n",
    "reviews_clean['review_score'] = reviews_clean['review_score'].fillna(median_score)\n",
    "\n",
    "# Create product category translation dict (sample of common ones)\n",
    "category_translation = {\n",
    "    'beleza_saude': 'health_beauty',\n",
    "    'informatica_acessorios': 'computers_accessories',\n",
    "    'automotivo': 'automotive',\n",
    "    'cama_mesa_banho': 'bed_bath_table',\n",
    "    'moveis_decoracao': 'furniture_decor',\n",
    "    'esporte_lazer': 'sports_leisure',\n",
    "    'perfumaria': 'perfumery',\n",
    "    'utilidades_domesticas': 'housewares',\n",
    "    'telefonia': 'telephony',\n",
    "    'relogios_presentes': 'watches_gifts',\n",
    "    'ferramentas_jardim': 'garden_tools',\n",
    "    'fashion_moda_praia': 'fashion_beach',\n",
    "    'cool_stuff': 'cool_stuff',\n",
    "    'brinquedos': 'toys',\n",
    "    'construcao_ferramentas_construcao': 'construction_tools',\n",
    "    'eletroportateis': 'small_appliances',\n",
    "    'casa_construcao': 'home_construction',\n",
    "    'instrumentos_musicais': 'musical_instruments',\n",
    "    'eletrodomesticos': 'home_appliances',\n",
    "    'livros_interesse_geral': 'books_general_interest'\n",
    "}\n",
    "\n",
    "# Apply translation where available\n",
    "products['product_category_name_english'] = products['product_category_name'].map(category_translation)\n",
    "products['product_category_name_english'] = products['product_category_name_english'].fillna(products['product_category_name'])\n",
    "\n",
    "# Clean geolocation data - remove duplicates and outliers\n",
    "geo_clean = geolocation.drop_duplicates(subset=['geolocation_zip_code_prefix'])\n",
    "\n",
    "# Remove obvious outliers in coordinates (focus on Brazil)\n",
    "geo_clean = geo_clean[\n",
    "    (geo_clean['geolocation_lat'].between(-35, 10)) &\n",
    "    (geo_clean['geolocation_lng'].between(-75, -30))\n",
    "]\n",
    "\n",
    "print(f\"Geolocation records after cleaning: {len(geo_clean)}\")\n",
    "\n",
    "# Verify data integrity\n",
    "print(\"\\nData integrity checks:\")\n",
    "print(f\"Orders with items: {orders_clean['order_id'].isin(order_items_clean['order_id']).sum()}\")\n",
    "print(f\"Orders with payments: {orders_clean['order_id'].isin(payments_clean['order_id']).sum()}\")\n",
    "print(f\"Orders with reviews: {orders_clean['order_id'].isin(reviews_clean['order_id']).sum()}\")\n",
    "\n",
    "print(\"\\nCleaning completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24ef098-839d-4eeb-8a8a-07bbf80461ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building master dataset...\n",
      "Calculating customer behavior metrics...\n",
      "Creating RFM features...\n",
      "Master dataset shape: (96470, 44)\n",
      "Features created: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date', 'delivery_days', 'estimated_delivery_days', 'delivery_vs_estimate', 'on_time_delivery', 'order_year', 'order_month', 'order_dayofweek', 'order_quarter', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state_x', 'order_value', 'avg_item_price', 'item_count', 'freight_value', 'unique_products', 'total_payment', 'max_installments', 'primary_payment_type', 'review_score', 'has_review_comment', 'primary_category', 'total_orders', 'total_spent', 'avg_order_value', 'first_order_date', 'last_order_date', 'avg_review_score', 'on_time_delivery_rate', 'customer_tenure_days', 'days_since_last_order', 'recency', 'frequency', 'monetary', 'customer_state_y']\n",
      "Master dataset saved to master_dataset.csv\n",
      "\n",
      "Sample of engineered features:\n",
      "                           order_id  delivery_days  order_value  review_score  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7              8        29.99           4.0   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451             13       118.70           4.0   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d              9       159.90           5.0   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a             13        45.00           5.0   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159              2        19.90           5.0   \n",
      "\n",
      "   recency  frequency  monetary  \n",
      "0      331          2     65.38  \n",
      "1       35          1    118.70  \n",
      "2       21          1    159.90  \n",
      "3      283          1     45.00  \n",
      "4      196          1     19.90  \n"
     ]
    }
   ],
   "source": [
    "# Section 3: Feature Engineering\n",
    "\n",
    "# Create comprehensive order dataset\n",
    "print(\"Building master dataset...\")\n",
    "\n",
    "# Start with orders and add delivery metrics\n",
    "master_df = orders_clean.copy()\n",
    "\n",
    "# Calculate delivery performance\n",
    "master_df['delivery_days'] = (master_df['order_delivered_customer_date'] - master_df['order_purchase_timestamp']).dt.days\n",
    "master_df['estimated_delivery_days'] = (pd.to_datetime(master_df['order_estimated_delivery_date']) - master_df['order_purchase_timestamp']).dt.days\n",
    "master_df['delivery_vs_estimate'] = master_df['delivery_days'] - master_df['estimated_delivery_days']\n",
    "master_df['on_time_delivery'] = (master_df['delivery_vs_estimate'] <= 0).astype(int)\n",
    "\n",
    "# Add temporal features\n",
    "master_df['order_year'] = master_df['order_purchase_timestamp'].dt.year\n",
    "master_df['order_month'] = master_df['order_purchase_timestamp'].dt.month\n",
    "master_df['order_dayofweek'] = master_df['order_purchase_timestamp'].dt.dayofweek\n",
    "master_df['order_quarter'] = master_df['order_purchase_timestamp'].dt.quarter\n",
    "\n",
    "# Add customer info\n",
    "master_df = master_df.merge(customers, on='customer_id', how='left')\n",
    "\n",
    "# Add order value and item metrics\n",
    "order_metrics = order_items_clean.groupby('order_id').agg({\n",
    "    'price': ['sum', 'mean', 'count'],\n",
    "    'freight_value': 'sum',\n",
    "    'product_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "order_metrics.columns = ['order_id', 'order_value', 'avg_item_price', 'item_count', 'freight_value', 'unique_products']\n",
    "master_df = master_df.merge(order_metrics, on='order_id', how='left')\n",
    "\n",
    "# Add payment info\n",
    "payment_metrics = payments_clean.groupby('order_id').agg({\n",
    "    'payment_value': 'sum',\n",
    "    'payment_installments': 'max',\n",
    "    'payment_type': lambda x: x.mode().iloc[0] if not x.empty else 'unknown'\n",
    "}).reset_index()\n",
    "\n",
    "payment_metrics.columns = ['order_id', 'total_payment', 'max_installments', 'primary_payment_type']\n",
    "master_df = master_df.merge(payment_metrics, on='order_id', how='left')\n",
    "\n",
    "# Add review data\n",
    "review_metrics = reviews_clean.groupby('order_id').agg({\n",
    "    'review_score': 'first',\n",
    "    'review_comment_message': lambda x: (x.notna()).sum()\n",
    "}).reset_index()\n",
    "\n",
    "review_metrics.columns = ['order_id', 'review_score', 'has_review_comment']\n",
    "master_df = master_df.merge(review_metrics, on='order_id', how='left')\n",
    "\n",
    "# Fill missing review scores with median\n",
    "master_df['review_score'] = master_df['review_score'].fillna(master_df['review_score'].median())\n",
    "\n",
    "# Add product category info\n",
    "order_categories = order_items_clean.merge(products[['product_id', 'product_category_name_english']], on='product_id', how='left')\n",
    "primary_category = order_categories.groupby('order_id')['product_category_name_english'].first().reset_index()\n",
    "primary_category.columns = ['order_id', 'primary_category']\n",
    "master_df = master_df.merge(primary_category, on='order_id', how='left')\n",
    "\n",
    "# Create customer behavior features\n",
    "print(\"Calculating customer behavior metrics...\")\n",
    "\n",
    "# Customer lifetime metrics\n",
    "customer_metrics = master_df.groupby('customer_unique_id').agg({\n",
    "    'order_id': 'count',\n",
    "    'order_value': ['sum', 'mean'],\n",
    "    'order_purchase_timestamp': ['min', 'max'],\n",
    "    'review_score': 'mean',\n",
    "    'on_time_delivery': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "customer_metrics.columns = ['customer_unique_id', 'total_orders', 'total_spent', 'avg_order_value', \n",
    "                           'first_order_date', 'last_order_date', 'avg_review_score', 'on_time_delivery_rate']\n",
    "\n",
    "# Calculate customer tenure and recency\n",
    "reference_date = master_df['order_purchase_timestamp'].max()\n",
    "customer_metrics['customer_tenure_days'] = (customer_metrics['last_order_date'] - customer_metrics['first_order_date']).dt.days\n",
    "customer_metrics['days_since_last_order'] = (reference_date - customer_metrics['last_order_date']).dt.days\n",
    "\n",
    "# Merge back to master dataset\n",
    "master_df = master_df.merge(customer_metrics, on='customer_unique_id', how='left')\n",
    "\n",
    "# Create RFM features\n",
    "print(\"Creating RFM features...\")\n",
    "master_df['recency'] = master_df['days_since_last_order']\n",
    "master_df['frequency'] = master_df['total_orders']\n",
    "master_df['monetary'] = master_df['total_spent']\n",
    "\n",
    "# Add geographic features\n",
    "geo_mapping = geo_clean.groupby('geolocation_zip_code_prefix').agg({\n",
    "    'geolocation_lat': 'mean',\n",
    "    'geolocation_lng': 'mean',\n",
    "    'geolocation_city': 'first',\n",
    "    'geolocation_state': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "customers_with_geo = customers.merge(geo_mapping, \n",
    "                                   left_on='customer_zip_code_prefix', \n",
    "                                   right_on='geolocation_zip_code_prefix', \n",
    "                                   how='left')\n",
    "\n",
    "# Add state info to master dataset\n",
    "state_info = customers_with_geo[['customer_id', 'geolocation_state']].rename(columns={'geolocation_state': 'customer_state'})\n",
    "master_df = master_df.merge(state_info, on='customer_id', how='left')\n",
    "\n",
    "print(f\"Master dataset shape: {master_df.shape}\")\n",
    "print(f\"Features created: {master_df.columns.tolist()}\")\n",
    "\n",
    "# Save checkpoint\n",
    "master_df.to_csv('master_dataset.csv', index=False)\n",
    "print(\"Master dataset saved to master_dataset.csv\")\n",
    "\n",
    "print(\"\\nSample of engineered features:\")\n",
    "print(master_df[['order_id', 'delivery_days', 'order_value', 'review_score', 'recency', 'frequency', 'monetary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7a40a6-6258-4369-acf8-c35899230713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RFM Analysis...\n",
      "Available columns in master_df:\n",
      "['customer_id', 'order_delivered_customer_date', 'delivery_days', 'estimated_delivery_days', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state_x', 'avg_item_price', 'total_payment', 'total_orders', 'total_spent', 'avg_order_value', 'avg_review_score', 'customer_tenure_days', 'days_since_last_order', 'customer_state_y']\n",
      "Using state column: customer_state_x\n",
      "Customers for RFM analysis: 93350\n",
      "Performing K-means clustering...\n",
      "Optimal number of clusters: 2\n",
      "\n",
      "Cluster Analysis:\n",
      "         avg_recency  customer_count  avg_frequency  avg_monetary  \\\n",
      "cluster                                                             \n",
      "0             237.50           90549           1.00        137.96   \n",
      "1             219.29            2801           2.11        260.05   \n",
      "\n",
      "         avg_review_score  \n",
      "cluster                    \n",
      "0                    4.16  \n",
      "1                    4.22  \n",
      "\n",
      "Cluster Labels:\n",
      "Cluster 0: At Risk/Lost (90549 customers)\n",
      "Cluster 1: Loyal Customers (2801 customers)\n",
      "\n",
      "Traditional RFM Segments:\n",
      "                     customer_count  avg_recency  avg_frequency  avg_monetary\n",
      "rfm_segment                                                                  \n",
      "Others                        55185       252.51           1.02        142.72\n",
      "Potential Loyalists            7286        60.84           1.03         49.33\n",
      "Loyal Customers                6010       178.05           1.09        252.55\n",
      "Champions                      5689        84.31           1.20        293.91\n",
      "New Customers                  5422       121.96           1.00         38.45\n",
      "At Risk                        5032       429.22           1.07        265.28\n",
      "Lost                           4908       474.33           1.00         30.90\n",
      "About to Sleep                 3818       273.00           1.00         26.15\n",
      "\n",
      "Top states by segment:\n",
      "\n",
      "At Risk/Lost:\n",
      "  SP: 37919 customers\n",
      "  RJ: 11523 customers\n",
      "  MG: 10674 customers\n",
      "\n",
      "Loyal Customers:\n",
      "  SP: 1224 customers\n",
      "  RJ: 390 customers\n",
      "  MG: 322 customers\n",
      "\n",
      "Customer segmentation completed. Results saved to customers_segmented.csv\n",
      "Total customers segmented: 93350\n",
      "\n",
      "Segmentation Summary:\n",
      "cluster_label\n",
      "At Risk/Lost       90549\n",
      "Loyal Customers     2801\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Section 4: Customer Segmentation using RFM Analysis\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Starting RFM Analysis...\")\n",
    "\n",
    "# First, let's check what columns we have\n",
    "print(\"Available columns in master_df:\")\n",
    "print([col for col in master_df.columns if any(keyword in col.lower() for keyword in ['customer', 'days', 'total', 'avg', 'state'])])\n",
    "\n",
    "# Create customer-level RFM dataset with available columns\n",
    "agg_dict = {\n",
    "    'days_since_last_order': 'first',  # Recency\n",
    "    'total_orders': 'first',           # Frequency  \n",
    "    'total_spent': 'first',            # Monetary\n",
    "    'avg_review_score': 'first'\n",
    "}\n",
    "\n",
    "# Add state column if it exists\n",
    "state_columns = [col for col in master_df.columns if 'state' in col.lower()]\n",
    "if state_columns:\n",
    "    agg_dict[state_columns[0]] = 'first'\n",
    "    print(f\"Using state column: {state_columns[0]}\")\n",
    "else:\n",
    "    print(\"No state column found, continuing without geographic data\")\n",
    "\n",
    "rfm_data = master_df.groupby('customer_unique_id').agg(agg_dict).reset_index()\n",
    "\n",
    "# Set column names dynamically\n",
    "base_columns = ['customer_id', 'recency', 'frequency', 'monetary', 'avg_review_score']\n",
    "if state_columns:\n",
    "    rfm_data.columns = base_columns + ['customer_state']\n",
    "else:\n",
    "    rfm_data.columns = base_columns\n",
    "\n",
    "# Remove customers with missing RFM values\n",
    "rfm_data = rfm_data.dropna(subset=['recency', 'frequency', 'monetary'])\n",
    "print(f\"Customers for RFM analysis: {len(rfm_data)}\")\n",
    "\n",
    "# Log transform monetary values to handle skewness\n",
    "rfm_data['monetary_log'] = np.log1p(rfm_data['monetary'])\n",
    "\n",
    "# Create RFM scores using quantiles\n",
    "rfm_data['R_score'] = pd.qcut(rfm_data['recency'], 5, labels=[5,4,3,2,1])  # Lower recency = higher score\n",
    "rfm_data['F_score'] = pd.qcut(rfm_data['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "rfm_data['M_score'] = pd.qcut(rfm_data['monetary'], 5, labels=[1,2,3,4,5])\n",
    "\n",
    "# Convert to numeric\n",
    "rfm_data['R_score'] = pd.to_numeric(rfm_data['R_score'])\n",
    "rfm_data['F_score'] = pd.to_numeric(rfm_data['F_score'])\n",
    "rfm_data['M_score'] = pd.to_numeric(rfm_data['M_score'])\n",
    "\n",
    "# Create combined RFM score\n",
    "rfm_data['RFM_score'] = rfm_data['R_score'].astype(str) + rfm_data['F_score'].astype(str) + rfm_data['M_score'].astype(str)\n",
    "\n",
    "# Traditional RFM segments\n",
    "def categorize_rfm(row):\n",
    "    if row['RFM_score'] in ['555', '554', '544', '545', '454', '455', '445']:\n",
    "        return 'Champions'\n",
    "    elif row['RFM_score'] in ['543', '444', '435', '355', '354', '345', '344', '335']:\n",
    "        return 'Loyal Customers'\n",
    "    elif row['RFM_score'] in ['553', '551', '552', '541', '542', '533', '532', '531', '452', '451']:\n",
    "        return 'Potential Loyalists'\n",
    "    elif row['RFM_score'] in ['512', '511', '422', '421', '412', '411', '311']:\n",
    "        return 'New Customers'\n",
    "    elif row['RFM_score'] in ['155', '154', '144', '214', '215', '115', '114']:\n",
    "        return 'At Risk'\n",
    "    elif row['RFM_score'] in ['155', '154', '144', '214', '215', '115']:\n",
    "        return 'Cannot Lose Them'\n",
    "    elif row['RFM_score'] in ['331', '321', '231', '241', '251']:\n",
    "        return 'About to Sleep'\n",
    "    elif row['RFM_score'] in ['111', '112', '121', '131', '141', '151']:\n",
    "        return 'Lost'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "rfm_data['rfm_segment'] = rfm_data.apply(categorize_rfm, axis=1)\n",
    "\n",
    "# K-means clustering for validation\n",
    "print(\"Performing K-means clustering...\")\n",
    "\n",
    "# Prepare data for clustering\n",
    "rfm_features = rfm_data[['recency', 'frequency', 'monetary_log']].copy()\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_features)\n",
    "\n",
    "# Find optimal number of clusters\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(rfm_scaled)\n",
    "    silhouette_avg = silhouette_score(rfm_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Apply optimal clustering\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "rfm_data['cluster'] = kmeans_final.fit_predict(rfm_scaled)\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nCluster Analysis:\")\n",
    "cluster_summary = rfm_data.groupby('cluster').agg({\n",
    "    'recency': ['mean', 'count'],\n",
    "    'frequency': 'mean',\n",
    "    'monetary': 'mean',\n",
    "    'avg_review_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "cluster_summary.columns = ['avg_recency', 'customer_count', 'avg_frequency', 'avg_monetary', 'avg_review_score']\n",
    "print(cluster_summary)\n",
    "\n",
    "# Create cluster labels based on characteristics\n",
    "def label_cluster(cluster_num):\n",
    "    cluster_data = rfm_data[rfm_data['cluster'] == cluster_num]\n",
    "    avg_recency = cluster_data['recency'].mean()\n",
    "    avg_frequency = cluster_data['frequency'].mean()\n",
    "    avg_monetary = cluster_data['monetary'].mean()\n",
    "    \n",
    "    if avg_recency < 100 and avg_frequency > 2 and avg_monetary > 300:\n",
    "        return 'High Value Active'\n",
    "    elif avg_recency < 100 and avg_frequency <= 2:\n",
    "        return 'Recent Customers'\n",
    "    elif avg_frequency > 2 and avg_monetary > 200:\n",
    "        return 'Loyal Customers'\n",
    "    elif avg_recency > 200:\n",
    "        return 'At Risk/Lost'\n",
    "    else:\n",
    "        return 'Standard Customers'\n",
    "\n",
    "cluster_labels = {i: label_cluster(i) for i in range(optimal_k)}\n",
    "rfm_data['cluster_label'] = rfm_data['cluster'].map(cluster_labels)\n",
    "\n",
    "print(\"\\nCluster Labels:\")\n",
    "for cluster_id, label in cluster_labels.items():\n",
    "    count = (rfm_data['cluster'] == cluster_id).sum()\n",
    "    print(f\"Cluster {cluster_id}: {label} ({count} customers)\")\n",
    "\n",
    "# Traditional RFM segment analysis\n",
    "print(\"\\nTraditional RFM Segments:\")\n",
    "segment_summary = rfm_data.groupby('rfm_segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean', \n",
    "    'monetary': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "segment_summary.columns = ['customer_count', 'avg_recency', 'avg_frequency', 'avg_monetary']\n",
    "segment_summary = segment_summary.sort_values('customer_count', ascending=False)\n",
    "print(segment_summary)\n",
    "\n",
    "# Geographic distribution by segment\n",
    "print(\"\\nTop states by segment:\")\n",
    "geo_segment = rfm_data.groupby(['customer_state', 'cluster_label']).size().reset_index(name='count')\n",
    "for label in rfm_data['cluster_label'].unique():\n",
    "    if pd.notna(label):\n",
    "        top_states = geo_segment[geo_segment['cluster_label'] == label].nlargest(3, 'count')\n",
    "        print(f\"\\n{label}:\")\n",
    "        for _, row in top_states.iterrows():\n",
    "            if pd.notna(row['customer_state']):\n",
    "                print(f\"  {row['customer_state']}: {row['count']} customers\")\n",
    "\n",
    "# Export results\n",
    "customers_segmented = rfm_data[['customer_id', 'recency', 'frequency', 'monetary', \n",
    "                               'rfm_segment', 'cluster', 'cluster_label', 'customer_state']].copy()\n",
    "\n",
    "customers_segmented.to_csv('customers_segmented.csv', index=False)\n",
    "print(f\"\\nCustomer segmentation completed. Results saved to customers_segmented.csv\")\n",
    "print(f\"Total customers segmented: {len(customers_segmented)}\")\n",
    "\n",
    "# Quick validation\n",
    "print(\"\\nSegmentation Summary:\")\n",
    "print(customers_segmented['cluster_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0540a163-6c6e-47cd-af16-497eb692ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building churn prediction model...\n",
      "Total customers: 93350\n",
      "Churned customers: 55004 (58.92%)\n",
      "Customers with sufficient history: 1457\n",
      "Missing values in features: 0\n",
      "Training set: 1165 samples\n",
      "Test set: 292 samples\n",
      "Training churn rate: 42.49%\n",
      "\n",
      "Model Comparison (Cross-Validation):\n",
      "Logistic Regression: 0.6349 (+/- 0.0936)\n",
      "Random Forest: 0.6056 (+/- 0.0681)\n",
      "Gradient Boosting: 0.6144 (+/- 0.0659)\n",
      "\n",
      "Best model: Logistic Regression\n",
      "Performing hyperparameter tuning...\n",
      "Best parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best CV score: 0.6431\n",
      "\n",
      "Model Performance:\n",
      "Test AUC: 0.6225\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67       168\n",
      "           1       0.50      0.35      0.42       124\n",
      "\n",
      "    accuracy                           0.58       292\n",
      "   macro avg       0.55      0.55      0.54       292\n",
      "weighted avg       0.56      0.58      0.56       292\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[124  44]\n",
      " [ 80  44]]\n",
      "\n",
      "Churn Risk Distribution:\n",
      "churn_risk_category\n",
      "Medium Risk    1132\n",
      "Low Risk        318\n",
      "High Risk         7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "High-risk customers: 7\n",
      "Potential revenue at risk: $76,353.53\n",
      "\n",
      "Churn predictions saved to churn_predictions.csv\n",
      "Total customers analyzed: 1457\n",
      "\n",
      "Key Insights:\n",
      "1. Model can identify churned customers with 62.3% accuracy (AUC)\n",
      "2. 7 customers are at high risk of churning\n",
      "3. Potential revenue at risk: $76,354\n",
      "Churn modeling completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Section 5: Churn Prediction Modeling\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Building churn prediction model...\")\n",
    "\n",
    "# Define churn based on recency (no orders in last 180 days)\n",
    "CHURN_THRESHOLD = 180\n",
    "\n",
    "# Create churn dataset from customer-level data\n",
    "churn_data = master_df.groupby('customer_unique_id').agg({\n",
    "    'days_since_last_order': 'first',\n",
    "    'total_orders': 'first',\n",
    "    'total_spent': 'first',\n",
    "    'avg_order_value': 'first',\n",
    "    'avg_review_score': 'first',\n",
    "    'on_time_delivery_rate': 'first',\n",
    "    'customer_tenure_days': 'first',\n",
    "    'customer_state_y': 'first',  # Using the correct column name\n",
    "    'primary_category': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'unknown'\n",
    "}).reset_index()\n",
    "\n",
    "churn_data.columns = ['customer_id', 'days_since_last_order', 'total_orders', 'total_spent', \n",
    "                     'avg_order_value', 'avg_review_score', 'on_time_delivery_rate', \n",
    "                     'customer_tenure_days', 'customer_state', 'primary_category']\n",
    "\n",
    "# Define churn target\n",
    "churn_data['is_churned'] = (churn_data['days_since_last_order'] > CHURN_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Total customers: {len(churn_data)}\")\n",
    "print(f\"Churned customers: {churn_data['is_churned'].sum()} ({churn_data['is_churned'].mean():.2%})\")\n",
    "\n",
    "# Feature engineering for churn model\n",
    "churn_data['order_frequency'] = churn_data['total_orders'] / (churn_data['customer_tenure_days'] + 1) * 30  # orders per month\n",
    "churn_data['clv_estimate'] = churn_data['avg_order_value'] * churn_data['order_frequency'] * 12  # annual CLV estimate\n",
    "\n",
    "# Handle missing values\n",
    "numeric_columns = ['total_orders', 'total_spent', 'avg_order_value', 'avg_review_score', \n",
    "                  'on_time_delivery_rate', 'customer_tenure_days', 'order_frequency', 'clv_estimate']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    churn_data[col] = churn_data[col].fillna(churn_data[col].median())\n",
    "\n",
    "# Fill categorical missing values\n",
    "churn_data['customer_state'] = churn_data['customer_state'].fillna('Unknown')\n",
    "churn_data['primary_category'] = churn_data['primary_category'].fillna('unknown')\n",
    "\n",
    "# Encode categorical variables\n",
    "le_state = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "\n",
    "churn_data['state_encoded'] = le_state.fit_transform(churn_data['customer_state'])\n",
    "churn_data['category_encoded'] = le_category.fit_transform(churn_data['primary_category'])\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = ['total_orders', 'total_spent', 'avg_order_value', 'avg_review_score', \n",
    "                  'on_time_delivery_rate', 'customer_tenure_days', 'order_frequency', \n",
    "                  'clv_estimate', 'state_encoded', 'category_encoded']\n",
    "\n",
    "# Remove customers with insufficient history (less than 30 days tenure)\n",
    "model_data = churn_data[churn_data['customer_tenure_days'] >= 30].copy()\n",
    "print(f\"Customers with sufficient history: {len(model_data)}\")\n",
    "\n",
    "X = model_data[feature_columns]\n",
    "y = model_data['is_churned']\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"Missing values in features: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Training churn rate: {y_train.mean():.2%}\")\n",
    "\n",
    "# Model comparison\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "print(\"\\nModel Comparison (Cross-Validation):\")\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    model_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'model': model\n",
    "    }\n",
    "    print(f\"{name}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['cv_mean'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# Hyperparameter tuning for best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "else:  # Logistic Regression\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "print(\"Performing hyperparameter tuning...\")\n",
    "grid_search = GridSearchCV(best_model, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train final model and make predictions\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance analysis\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': final_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Feature Importance for Churn Prediction')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create predictions for all customers\n",
    "all_predictions = final_model.predict_proba(X)[:, 1]\n",
    "model_data['churn_probability'] = all_predictions\n",
    "model_data['churn_risk_category'] = pd.cut(all_predictions, \n",
    "                                          bins=[0, 0.3, 0.7, 1.0], \n",
    "                                          labels=['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "\n",
    "print(\"\\nChurn Risk Distribution:\")\n",
    "print(model_data['churn_risk_category'].value_counts())\n",
    "\n",
    "# Business insights\n",
    "high_risk_customers = model_data[model_data['churn_risk_category'] == 'High Risk']\n",
    "print(f\"\\nHigh-risk customers: {len(high_risk_customers)}\")\n",
    "print(f\"Potential revenue at risk: ${high_risk_customers['clv_estimate'].sum():,.2f}\")\n",
    "\n",
    "# Export churn predictions\n",
    "churn_predictions = model_data[['customer_id', 'is_churned', 'churn_probability', \n",
    "                               'churn_risk_category', 'total_spent', 'clv_estimate',\n",
    "                               'customer_state', 'primary_category']].copy()\n",
    "\n",
    "churn_predictions.to_csv('churn_predictions.csv', index=False)\n",
    "print(f\"\\nChurn predictions saved to churn_predictions.csv\")\n",
    "print(f\"Total customers analyzed: {len(churn_predictions)}\")\n",
    "\n",
    "# Model insights summary\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"1. Model can identify churned customers with {roc_auc_score(y_test, y_pred_proba):.1%} accuracy (AUC)\")\n",
    "print(f\"2. {len(high_risk_customers)} customers are at high risk of churning\")\n",
    "print(f\"3. Potential revenue at risk: ${high_risk_customers['clv_estimate'].sum():,.0f}\")\n",
    "\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    top_feature = feature_importance.iloc[0]['feature']\n",
    "    print(f\"4. Most important predictor: {top_feature}\")\n",
    "\n",
    "print(\"Churn modeling completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96aa756b-6d68-41d9-82f9-25d290b50fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing product performance and sales patterns...\n",
      "Analyzing category performance...\n",
      "Top 10 Categories by Revenue:\n",
      "                               total_revenue  items_sold  avg_review_score  \\\n",
      "product_category_name_english                                                \n",
      "health_beauty                     1237439.95        9519              4.19   \n",
      "watches_gifts                     1166968.63        5867              4.07   \n",
      "bed_bath_table                    1037177.69       11107              3.92   \n",
      "sports_leisure                     960010.09        8488              4.17   \n",
      "computers_accessories              896132.29        7707              3.99   \n",
      "furniture_decor                    718344.78        8239              3.95   \n",
      "housewares                         617836.73        6819              4.11   \n",
      "cool_stuff                         612071.86        3727              4.19   \n",
      "automotive                         580146.14        4157              4.12   \n",
      "toys                               471920.79        4037              4.21   \n",
      "\n",
      "                               market_share  \n",
      "product_category_name_english                \n",
      "health_beauty                          9.44  \n",
      "watches_gifts                          8.90  \n",
      "bed_bath_table                         7.91  \n",
      "sports_leisure                         7.32  \n",
      "computers_accessories                  6.84  \n",
      "furniture_decor                        5.48  \n",
      "housewares                             4.71  \n",
      "cool_stuff                             4.67  \n",
      "automotive                             4.43  \n",
      "toys                                   3.60  \n",
      "\n",
      "Analyzing seasonal patterns...\n",
      "Monthly sales trends (last 6 months):\n",
      "    order_year  order_month    revenue  orders  revenue_growth\n",
      "17        2018            3  956923.96    7003       14.502852\n",
      "18        2018            4  975779.41    6798        1.970423\n",
      "19        2018            5  978065.68    6749        0.234302\n",
      "20        2018            6  856423.90    6096      -12.436975\n",
      "21        2018            7  869375.49    6156        1.512287\n",
      "22        2018            8  838650.76    6351       -3.534115\n",
      "\n",
      "Categories with highest seasonal variation:\n",
      "product_category_name_english\n",
      "health_beauty            91780.204840\n",
      "computers_accessories    72126.231445\n",
      "watches_gifts            63196.858211\n",
      "housewares               62004.149341\n",
      "bed_bath_table           52874.351108\n",
      "sports_leisure           51120.534365\n",
      "automotive               41275.355978\n",
      "pcs                      39476.800043\n",
      "furniture_decor          35508.545412\n",
      "bebes                    30633.193154\n",
      "Name: price, dtype: float64\n",
      "\n",
      "Analyzing geographic performance...\n",
      "Top 10 States by Revenue:\n",
      "                total_revenue  unique_orders  avg_order_value  market_share\n",
      "customer_state                                                             \n",
      "SP                 5092309.32          40494           125.75         38.35\n",
      "RJ                 1766256.97          12350           143.02         13.30\n",
      "MG                 1558668.28          11354           137.28         11.74\n",
      "RS                  732241.80           5344           137.02          5.51\n",
      "PR                  668891.26           4923           135.87          5.04\n",
      "SC                  508578.90           3546           143.42          3.83\n",
      "BA                  495416.93           3256           152.16          3.73\n",
      "DF                  298492.74           2080           143.51          2.25\n",
      "GO                  285780.24           1957           146.03          2.15\n",
      "ES                  269230.52           1995           134.95          2.03\n",
      "\n",
      "Segmenting products by performance...\n",
      "Product Performance Segments:\n",
      "                     product_count  total_revenue  avg_review_score\n",
      "performance_segment                                                \n",
      "Premium Products              2989     2257497.97              4.13\n",
      "Standard Products            13493     1857150.01              4.12\n",
      "Star Products                 5065     8268848.17              4.08\n",
      "Underperformers               6823      238510.71              4.17\n",
      "Volume Products               3844      656580.55              4.06\n",
      "\n",
      "Exporting analysis results...\n",
      "Analysis completed successfully!\n",
      "Files exported:\n",
      "- product_performance.csv (73 categories)\n",
      "- seasonal_trends.csv (23 months)\n",
      "- geographic_analysis.csv (27 states)\n",
      "- business_insights.csv (key metrics)\n",
      "\n",
      "Key Business Insights:\n",
      "1. Top performing category: health_beauty ($1,237,440)\n",
      "2. Top performing state: SP ($5,092,309)\n",
      "3. Peak sales month: 11\n",
      "4. Star products identified: 5065\n",
      "5. Overall customer satisfaction: 4.08/5.0\n"
     ]
    }
   ],
   "source": [
    "# Section 6: Product and Sales Analysis\n",
    "\n",
    "print(\"Analyzing product performance and sales patterns...\")\n",
    "\n",
    "# Create comprehensive product analysis dataset\n",
    "product_analysis = order_items_clean.merge(products[['product_id', 'product_category_name_english']], \n",
    "                                         on='product_id', how='left')\n",
    "product_analysis = product_analysis.merge(orders_clean[['order_id', 'customer_id', 'order_purchase_timestamp']], \n",
    "                                        on='order_id', how='left')\n",
    "# Add customer state by merging with customers table\n",
    "product_analysis = product_analysis.merge(customers[['customer_id', 'customer_state']], \n",
    "                                        on='customer_id', how='left')\n",
    "product_analysis = product_analysis.merge(reviews_clean[['order_id', 'review_score']], \n",
    "                                        on='order_id', how='left')\n",
    "\n",
    "# Add temporal features\n",
    "product_analysis['order_year'] = pd.to_datetime(product_analysis['order_purchase_timestamp']).dt.year\n",
    "product_analysis['order_month'] = pd.to_datetime(product_analysis['order_purchase_timestamp']).dt.month\n",
    "product_analysis['order_quarter'] = pd.to_datetime(product_analysis['order_purchase_timestamp']).dt.quarter\n",
    "\n",
    "# Category performance analysis\n",
    "print(\"Analyzing category performance...\")\n",
    "category_performance = product_analysis.groupby('product_category_name_english').agg({\n",
    "    'price': ['sum', 'mean', 'count'],\n",
    "    'freight_value': ['sum', 'mean'],\n",
    "    'review_score': 'mean',\n",
    "    'product_id': 'nunique',\n",
    "    'order_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "category_performance.columns = ['total_revenue', 'avg_price', 'items_sold', 'total_freight', \n",
    "                               'avg_freight', 'avg_review_score', 'unique_products', 'unique_orders']\n",
    "\n",
    "# Calculate profitability metrics (assuming freight is a cost)\n",
    "category_performance['gross_revenue'] = category_performance['total_revenue']\n",
    "category_performance['estimated_profit'] = category_performance['total_revenue'] - category_performance['total_freight']\n",
    "category_performance['profit_margin'] = (category_performance['estimated_profit'] / category_performance['total_revenue'] * 100).round(2)\n",
    "\n",
    "# Add market share\n",
    "total_revenue = category_performance['total_revenue'].sum()\n",
    "category_performance['market_share'] = (category_performance['total_revenue'] / total_revenue * 100).round(2)\n",
    "\n",
    "category_performance = category_performance.sort_values('total_revenue', ascending=False)\n",
    "\n",
    "print(\"Top 10 Categories by Revenue:\")\n",
    "print(category_performance.head(10)[['total_revenue', 'items_sold', 'avg_review_score', 'market_share']])\n",
    "\n",
    "# Seasonal analysis\n",
    "print(\"\\nAnalyzing seasonal patterns...\")\n",
    "monthly_sales = product_analysis.groupby(['order_year', 'order_month']).agg({\n",
    "    'price': 'sum',\n",
    "    'order_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "monthly_sales['revenue'] = monthly_sales['price']\n",
    "monthly_sales['orders'] = monthly_sales['order_id']\n",
    "monthly_sales['date'] = pd.to_datetime(monthly_sales[['order_year', 'order_month']].rename(columns={'order_year': 'year', 'order_month': 'month'}).assign(day=1))\n",
    "\n",
    "# Calculate month-over-month growth\n",
    "monthly_sales = monthly_sales.sort_values('date')\n",
    "monthly_sales['revenue_growth'] = monthly_sales['revenue'].pct_change() * 100\n",
    "monthly_sales['orders_growth'] = monthly_sales['orders'].pct_change() * 100\n",
    "\n",
    "print(\"Monthly sales trends (last 6 months):\")\n",
    "print(monthly_sales.tail(6)[['order_year', 'order_month', 'revenue', 'orders', 'revenue_growth']])\n",
    "\n",
    "# Seasonal category analysis\n",
    "seasonal_categories = product_analysis.groupby(['product_category_name_english', 'order_quarter']).agg({\n",
    "    'price': 'sum',\n",
    "    'order_id': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Find categories with highest seasonal variation\n",
    "category_seasonal_variation = seasonal_categories.groupby('product_category_name_english')['price'].std().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nCategories with highest seasonal variation:\")\n",
    "print(category_seasonal_variation.head(10))\n",
    "\n",
    "# Geographic analysis\n",
    "print(\"\\nAnalyzing geographic performance...\")\n",
    "geographic_performance = product_analysis.groupby('customer_state').agg({\n",
    "    'price': ['sum', 'count'],\n",
    "    'review_score': 'mean',\n",
    "    'order_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "geographic_performance.columns = ['total_revenue', 'items_sold', 'avg_review_score', 'unique_orders']\n",
    "geographic_performance['avg_order_value'] = (geographic_performance['total_revenue'] / geographic_performance['unique_orders']).round(2)\n",
    "\n",
    "# Add market share by state\n",
    "total_revenue = geographic_performance['total_revenue'].sum()\n",
    "geographic_performance['market_share'] = (geographic_performance['total_revenue'] / total_revenue * 100).round(2)\n",
    "\n",
    "geographic_performance = geographic_performance.sort_values('total_revenue', ascending=False)\n",
    "\n",
    "print(\"Top 10 States by Revenue:\")\n",
    "print(geographic_performance.head(10)[['total_revenue', 'unique_orders', 'avg_order_value', 'market_share']])\n",
    "\n",
    "# Product performance segmentation\n",
    "print(\"\\nSegmenting products by performance...\")\n",
    "\n",
    "# Calculate product-level metrics\n",
    "product_metrics = product_analysis.groupby('product_id').agg({\n",
    "    'price': ['sum', 'count'],\n",
    "    'review_score': 'mean',\n",
    "    'order_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "product_metrics.columns = ['total_revenue', 'units_sold', 'avg_review_score', 'unique_orders']\n",
    "product_metrics = product_metrics.reset_index()\n",
    "\n",
    "# Create performance segments\n",
    "revenue_quartiles = product_metrics['total_revenue'].quantile([0.25, 0.5, 0.75, 1.0])\n",
    "units_quartiles = product_metrics['units_sold'].quantile([0.25, 0.5, 0.75, 1.0])\n",
    "\n",
    "def categorize_product(row):\n",
    "    revenue = row['total_revenue']\n",
    "    units = row['units_sold']\n",
    "    \n",
    "    high_revenue = revenue >= revenue_quartiles[0.75]\n",
    "    high_volume = units >= units_quartiles[0.75]\n",
    "    low_revenue = revenue <= revenue_quartiles[0.25]\n",
    "    low_volume = units <= units_quartiles[0.25]\n",
    "    \n",
    "    if high_revenue and high_volume:\n",
    "        return 'Star Products'\n",
    "    elif high_revenue and not high_volume:\n",
    "        return 'Premium Products' \n",
    "    elif not high_revenue and high_volume:\n",
    "        return 'Volume Products'\n",
    "    elif low_revenue and low_volume:\n",
    "        return 'Underperformers'\n",
    "    else:\n",
    "        return 'Standard Products'\n",
    "\n",
    "product_metrics['performance_segment'] = product_metrics.apply(categorize_product, axis=1)\n",
    "\n",
    "print(\"Product Performance Segments:\")\n",
    "segment_summary = product_metrics.groupby('performance_segment').agg({\n",
    "    'product_id': 'count',\n",
    "    'total_revenue': 'sum',\n",
    "    'avg_review_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "segment_summary.columns = ['product_count', 'total_revenue', 'avg_review_score']\n",
    "print(segment_summary)\n",
    "\n",
    "# Export comprehensive results\n",
    "print(\"\\nExporting analysis results...\")\n",
    "\n",
    "# Category performance export\n",
    "category_performance_export = category_performance.reset_index()\n",
    "category_performance_export.to_csv('product_performance.csv', index=False)\n",
    "\n",
    "# Monthly trends export\n",
    "monthly_sales.to_csv('seasonal_trends.csv', index=False)\n",
    "\n",
    "# Geographic analysis export\n",
    "geographic_performance_export = geographic_performance.reset_index()\n",
    "geographic_performance_export.to_csv('geographic_analysis.csv', index=False)\n",
    "\n",
    "# Combined insights dataset\n",
    "insights_summary = {\n",
    "    'top_category': category_performance.index[0],\n",
    "    'top_category_revenue': category_performance.iloc[0]['total_revenue'],\n",
    "    'total_categories': len(category_performance),\n",
    "    'avg_review_score_overall': product_analysis['review_score'].mean(),\n",
    "    'top_state': geographic_performance.index[0],\n",
    "    'top_state_revenue': geographic_performance.iloc[0]['total_revenue'],\n",
    "    'seasonal_peak_month': monthly_sales.loc[monthly_sales['revenue'].idxmax(), 'order_month'],\n",
    "    'total_products': len(product_metrics),\n",
    "    'star_products_count': (product_metrics['performance_segment'] == 'Star Products').sum()\n",
    "}\n",
    "\n",
    "insights_df = pd.DataFrame([insights_summary])\n",
    "insights_df.to_csv('business_insights.csv', index=False)\n",
    "\n",
    "print(f\"Analysis completed successfully!\")\n",
    "print(f\"Files exported:\")\n",
    "print(f\"- product_performance.csv ({len(category_performance_export)} categories)\")\n",
    "print(f\"- seasonal_trends.csv ({len(monthly_sales)} months)\")\n",
    "print(f\"- geographic_analysis.csv ({len(geographic_performance_export)} states)\")\n",
    "print(f\"- business_insights.csv (key metrics)\")\n",
    "\n",
    "print(f\"\\nKey Business Insights:\")\n",
    "print(f\"1. Top performing category: {insights_summary['top_category']} (${insights_summary['top_category_revenue']:,.0f})\")\n",
    "print(f\"2. Top performing state: {insights_summary['top_state']} (${insights_summary['top_state_revenue']:,.0f})\")\n",
    "print(f\"3. Peak sales month: {insights_summary['seasonal_peak_month']}\")\n",
    "print(f\"4. Star products identified: {insights_summary['star_products_count']}\")\n",
    "print(f\"5. Overall customer satisfaction: {insights_summary['avg_review_score_overall']:.2f}/5.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c958faec-85b1-4bdb-bdbb-9552a7e5c114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing final datasets for Tableau integration...\n",
      "Building comprehensive customer dataset...\n",
      "Customer master dataset: (93350, 13)\n",
      "Building comprehensive order dataset...\n",
      "Order master dataset: (96470, 48)\n",
      "Building time series dataset...\n",
      "Daily metrics dataset: (612, 14)\n",
      "Building category trends dataset...\n",
      "Category trends dataset: (1252, 9)\n",
      "Creating executive summary dataset...\n",
      "Executive Summary:\n",
      "total_customers: 93,350\n",
      "total_orders: 96,470\n",
      "total_revenue: $13,220,248.93\n",
      "avg_order_value: $137.04\n",
      "avg_review_score: 4\n",
      "on_time_delivery_rate: 92.43%\n",
      "customers_at_risk: 7\n",
      "revenue_at_risk: $76,353.53\n",
      "top_customer_segment: At Risk/Lost\n",
      "top_category: health_beauty\n",
      "top_state: SP\n",
      "analysis_date: 2018-08-29\n",
      "\n",
      "Exporting final datasets...\n",
      "✓ customer_master_tableau.csv: 93350 rows, 13 columns\n",
      "✓ order_master_tableau.csv: 96470 rows, 48 columns\n",
      "✓ daily_metrics_tableau.csv: 612 rows, 14 columns\n",
      "✓ category_trends_tableau.csv: 1252 rows, 9 columns\n",
      "✓ executive_summary_tableau.csv: 1 rows, 12 columns\n",
      "\n",
      "Creating data dictionary...\n",
      "✓ tableau_data_dictionary.csv: Data definitions for Tableau users\n",
      "\n",
      "Data validation summary:\n",
      "Customer master: 93350 customers\n",
      "Order master: 96470 orders\n",
      "Daily metrics: 612 days of data\n",
      "Category trends: 1252 category-month combinations\n",
      "Warning: Missing values in customer_master:\n",
      "churn_probability    91893\n",
      "dtype: int64\n",
      "✓ order_master: No missing values in key columns\n",
      "\n",
      "==================================================\n",
      "DATA PREPARATION COMPLETED SUCCESSFULLY!\n",
      "==================================================\n",
      "\n",
      "Next steps:\n",
      "1. Import the CSV files into Tableau\n",
      "2. Create relationships between datasets using customer_id and order_id\n",
      "3. Build the four recommended dashboards:\n",
      "   - Executive Dashboard\n",
      "   - Customer Analytics Dashboard\n",
      "   - Product Performance Dashboard\n",
      "   - Predictive Insights Dashboard\n",
      "\n",
      "All files are ready for professional Tableau visualization!\n"
     ]
    }
   ],
   "source": [
    "# Section 7: Final Data Integration and Tableau Export Preparation\n",
    "\n",
    "print(\"Preparing final datasets for Tableau integration...\")\n",
    "\n",
    "# Create master customer dataset with all insights\n",
    "print(\"Building comprehensive customer dataset...\")\n",
    "\n",
    "# Load our previously created datasets\n",
    "customers_segmented = pd.read_csv('customers_segmented.csv')\n",
    "churn_predictions = pd.read_csv('churn_predictions.csv')\n",
    "\n",
    "# Merge customer insights\n",
    "customer_master = customers_segmented.merge(\n",
    "    churn_predictions[['customer_id', 'churn_probability', 'churn_risk_category', 'clv_estimate']], \n",
    "    on='customer_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add geographic revenue data\n",
    "customer_revenue_by_state = geographic_performance_export.reset_index()\n",
    "customer_master = customer_master.merge(\n",
    "    customer_revenue_by_state[['customer_state', 'total_revenue', 'market_share']].rename(columns={\n",
    "        'total_revenue': 'state_total_revenue',\n",
    "        'market_share': 'state_market_share'\n",
    "    }),\n",
    "    on='customer_state',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Customer master dataset: {customer_master.shape}\")\n",
    "\n",
    "# Create order-level dataset for detailed analysis\n",
    "print(\"Building comprehensive order dataset...\")\n",
    "\n",
    "# Start with our clean master dataset\n",
    "order_master = master_df.copy()\n",
    "\n",
    "# Add customer segment information\n",
    "order_master = order_master.merge(\n",
    "    customers_segmented[['customer_id', 'cluster_label', 'rfm_segment']], \n",
    "    on='customer_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add churn risk information\n",
    "order_master = order_master.merge(\n",
    "    churn_predictions[['customer_id', 'churn_probability', 'churn_risk_category']], \n",
    "    left_on='customer_unique_id',\n",
    "    right_on='customer_id',\n",
    "    how='left',\n",
    "    suffixes=('', '_churn')\n",
    ")\n",
    "\n",
    "# Clean up duplicate customer_id column\n",
    "if 'customer_id_churn' in order_master.columns:\n",
    "    order_master = order_master.drop('customer_id_churn', axis=1)\n",
    "\n",
    "print(f\"Order master dataset: {order_master.shape}\")\n",
    "\n",
    "# Create time series dataset for trend analysis\n",
    "print(\"Building time series dataset...\")\n",
    "\n",
    "# Daily aggregations\n",
    "daily_metrics = order_master.groupby(order_master['order_purchase_timestamp'].dt.date).agg({\n",
    "    'order_id': 'count',\n",
    "    'order_value': ['sum', 'mean'],\n",
    "    'review_score': 'mean',\n",
    "    'on_time_delivery': 'mean',\n",
    "    'customer_unique_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "daily_metrics.columns = ['date', 'daily_orders', 'daily_revenue', 'avg_order_value', \n",
    "                        'avg_review_score', 'on_time_delivery_rate', 'unique_customers']\n",
    "\n",
    "# Add temporal features\n",
    "daily_metrics['date'] = pd.to_datetime(daily_metrics['date'])\n",
    "daily_metrics['year'] = daily_metrics['date'].dt.year\n",
    "daily_metrics['month'] = daily_metrics['date'].dt.month\n",
    "daily_metrics['quarter'] = daily_metrics['date'].dt.quarter\n",
    "daily_metrics['day_of_week'] = daily_metrics['date'].dt.day_name()\n",
    "daily_metrics['is_weekend'] = daily_metrics['date'].dt.weekday >= 5\n",
    "\n",
    "# Calculate rolling averages\n",
    "daily_metrics = daily_metrics.sort_values('date')\n",
    "daily_metrics['revenue_7d_avg'] = daily_metrics['daily_revenue'].rolling(7).mean()\n",
    "daily_metrics['revenue_30d_avg'] = daily_metrics['daily_revenue'].rolling(30).mean()\n",
    "\n",
    "print(f\"Daily metrics dataset: {daily_metrics.shape}\")\n",
    "\n",
    "# Create category performance dataset with trends\n",
    "print(\"Building category trends dataset...\")\n",
    "\n",
    "category_trends = product_analysis.groupby(['product_category_name_english', 'order_year', 'order_month']).agg({\n",
    "    'price': 'sum',\n",
    "    'order_id': 'count',\n",
    "    'review_score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "category_trends.columns = ['category', 'year', 'month', 'revenue', 'orders', 'avg_review_score']\n",
    "category_trends['date'] = pd.to_datetime(category_trends[['year', 'month']].assign(day=1))\n",
    "\n",
    "# Calculate growth rates by category\n",
    "category_trends = category_trends.sort_values(['category', 'date'])\n",
    "category_trends['revenue_growth'] = category_trends.groupby('category')['revenue'].pct_change() * 100\n",
    "category_trends['orders_growth'] = category_trends.groupby('category')['orders'].pct_change() * 100\n",
    "\n",
    "print(f\"Category trends dataset: {category_trends.shape}\")\n",
    "\n",
    "# Create final business summary dataset\n",
    "print(\"Creating executive summary dataset...\")\n",
    "\n",
    "# Current date reference\n",
    "latest_date = order_master['order_purchase_timestamp'].max()\n",
    "current_month = latest_date.month\n",
    "current_year = latest_date.year\n",
    "\n",
    "# Executive KPIs\n",
    "executive_summary = {\n",
    "    'total_customers': order_master['customer_unique_id'].nunique(),\n",
    "    'total_orders': len(order_master),\n",
    "    'total_revenue': order_master['order_value'].sum(),\n",
    "    'avg_order_value': order_master['order_value'].mean(),\n",
    "    'avg_review_score': order_master['review_score'].mean(),\n",
    "    'on_time_delivery_rate': order_master['on_time_delivery'].mean(),\n",
    "    'customers_at_risk': len(churn_predictions[churn_predictions['churn_risk_category'] == 'High Risk']),\n",
    "    'revenue_at_risk': churn_predictions[churn_predictions['churn_risk_category'] == 'High Risk']['clv_estimate'].sum(),\n",
    "    'top_customer_segment': customer_master['cluster_label'].mode().iloc[0],\n",
    "    'top_category': category_performance_export.iloc[0]['product_category_name_english'],\n",
    "    'top_state': geographic_performance_export.iloc[0]['customer_state'],\n",
    "    'analysis_date': latest_date.strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "executive_df = pd.DataFrame([executive_summary])\n",
    "\n",
    "print(\"Executive Summary:\")\n",
    "for key, value in executive_summary.items():\n",
    "    if isinstance(value, (int, float)) and 'rate' in key:\n",
    "        print(f\"{key}: {value:.2%}\")\n",
    "    elif isinstance(value, (int, float)) and 'revenue' in key or 'value' in key:\n",
    "        print(f\"{key}: ${value:,.2f}\")\n",
    "    elif isinstance(value, (int, float)):\n",
    "        print(f\"{key}: {value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Export all datasets for Tableau\n",
    "print(\"\\nExporting final datasets...\")\n",
    "\n",
    "datasets_to_export = {\n",
    "    'customer_master_tableau.csv': customer_master,\n",
    "    'order_master_tableau.csv': order_master,\n",
    "    'daily_metrics_tableau.csv': daily_metrics,\n",
    "    'category_trends_tableau.csv': category_trends,\n",
    "    'executive_summary_tableau.csv': executive_df\n",
    "}\n",
    "\n",
    "for filename, dataset in datasets_to_export.items():\n",
    "    # Clean column names for Tableau compatibility\n",
    "    dataset.columns = [col.replace(' ', '_').replace('(', '').replace(')', '').lower() \n",
    "                      for col in dataset.columns]\n",
    "    \n",
    "    # Handle datetime columns\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == 'datetime64[ns]':\n",
    "            dataset[col] = dataset[col].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    dataset.to_csv(filename, index=False)\n",
    "    print(f\"✓ {filename}: {dataset.shape[0]} rows, {dataset.shape[1]} columns\")\n",
    "\n",
    "# Create data dictionary for Tableau users\n",
    "print(\"\\nCreating data dictionary...\")\n",
    "\n",
    "data_dictionary = {\n",
    "    'Dataset': [],\n",
    "    'Column': [],\n",
    "    'Description': [],\n",
    "    'Data_Type': []\n",
    "}\n",
    "\n",
    "# Customer master dictionary\n",
    "customer_columns = {\n",
    "    'customer_id': 'Unique customer identifier',\n",
    "    'recency': 'Days since last purchase',\n",
    "    'frequency': 'Total number of orders',\n",
    "    'monetary': 'Total customer spend',\n",
    "    'rfm_segment': 'Traditional RFM customer segment',\n",
    "    'cluster_label': 'ML-based customer segment',\n",
    "    'churn_probability': 'Probability of customer churn (0-1)',\n",
    "    'churn_risk_category': 'Churn risk level (Low/Medium/High)',\n",
    "    'clv_estimate': 'Estimated customer lifetime value'\n",
    "}\n",
    "\n",
    "for col, desc in customer_columns.items():\n",
    "    data_dictionary['Dataset'].append('customer_master_tableau.csv')\n",
    "    data_dictionary['Column'].append(col)\n",
    "    data_dictionary['Description'].append(desc)\n",
    "    data_dictionary['Data_Type'].append('String' if col in ['rfm_segment', 'cluster_label', 'churn_risk_category'] else 'Number')\n",
    "\n",
    "# Order master key columns\n",
    "order_columns = {\n",
    "    'order_id': 'Unique order identifier',\n",
    "    'order_value': 'Total order value in BRL',\n",
    "    'review_score': 'Customer review score (1-5)',\n",
    "    'delivery_days': 'Actual delivery time in days',\n",
    "    'on_time_delivery': 'Whether delivered on time (1/0)',\n",
    "    'cluster_label': 'Customer segment',\n",
    "    'churn_probability': 'Customer churn probability'\n",
    "}\n",
    "\n",
    "for col, desc in order_columns.items():\n",
    "    data_dictionary['Dataset'].append('order_master_tableau.csv')\n",
    "    data_dictionary['Column'].append(col)\n",
    "    data_dictionary['Description'].append(desc)\n",
    "    data_dictionary['Data_Type'].append('String' if col == 'cluster_label' else 'Number')\n",
    "\n",
    "data_dict_df = pd.DataFrame(data_dictionary)\n",
    "data_dict_df.to_csv('tableau_data_dictionary.csv', index=False)\n",
    "\n",
    "print(\"✓ tableau_data_dictionary.csv: Data definitions for Tableau users\")\n",
    "\n",
    "# Final validation\n",
    "print(\"\\nData validation summary:\")\n",
    "print(f\"Customer master: {len(customer_master)} customers\")\n",
    "print(f\"Order master: {len(order_master)} orders\") \n",
    "print(f\"Daily metrics: {len(daily_metrics)} days of data\")\n",
    "print(f\"Category trends: {len(category_trends)} category-month combinations\")\n",
    "\n",
    "# Check for missing values in key columns\n",
    "key_columns_check = {\n",
    "    'customer_master': ['customer_id', 'cluster_label', 'churn_probability'],\n",
    "    'order_master': ['order_id', 'order_value', 'review_score']\n",
    "}\n",
    "\n",
    "for dataset_name, columns in key_columns_check.items():\n",
    "    dataset = customer_master if dataset_name == 'customer_master' else order_master\n",
    "    missing_summary = dataset[columns].isnull().sum()\n",
    "    if missing_summary.sum() > 0:\n",
    "        print(f\"Warning: Missing values in {dataset_name}:\")\n",
    "        print(missing_summary[missing_summary > 0])\n",
    "    else:\n",
    "        print(f\"✓ {dataset_name}: No missing values in key columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA PREPARATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Import the CSV files into Tableau\")\n",
    "print(\"2. Create relationships between datasets using customer_id and order_id\")\n",
    "print(\"3. Build the four recommended dashboards:\")\n",
    "print(\"   - Executive Dashboard\")\n",
    "print(\"   - Customer Analytics Dashboard\") \n",
    "print(\"   - Product Performance Dashboard\")\n",
    "print(\"   - Predictive Insights Dashboard\")\n",
    "print(\"\\nAll files are ready for professional Tableau visualization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665d4fd-acb9-406f-8b3c-80c76a06a8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
